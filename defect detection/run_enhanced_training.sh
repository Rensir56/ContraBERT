#!/bin/bash

# ContraBERT æ¼æ´žæ£€æµ‹ä¼˜åŒ–è®­ç»ƒè„šæœ¬
# ä½¿ç”¨å¤šç§å…ˆè¿›æŠ€æœ¯æå‡æ¨¡åž‹æ€§èƒ½

set -e  # é‡åˆ°é”™è¯¯æ—¶åœæ­¢æ‰§è¡Œ

export HF_ENDPOINT=https://hf-mirror.com

# =============================================================================
# é…ç½®å‚æ•°
# =============================================================================

# åŸºç¡€è·¯å¾„é…ç½®
PRETRAIN_DIR="../saved_models/pretrain_models/"
MODEL_TYPE="ContraBERT_C"  # æˆ– ContraBERT_G
DATA_DIR="./dataset"
OUTPUT_BASE="../saved_models/finetune_models/vulnerability_detection_enhanced"

# åˆ›å»ºè¾“å‡ºç›®å½•
OUTPUT_DIR="${OUTPUT_BASE}/${MODEL_TYPE}_enhanced_$(date +%Y%m%d_%H%M%S)"
mkdir -p ${OUTPUT_DIR}

echo "ðŸš€ å¼€å§‹ContraBERTå¢žå¼ºè®­ç»ƒ"
echo "ðŸ“ è¾“å‡ºç›®å½•: ${OUTPUT_DIR}"
echo "ðŸ¤– æ¨¡åž‹ç±»åž‹: ${MODEL_TYPE}"

# =============================================================================
# ç­–ç•¥1: åŸºç¡€å¢žå¼ºè®­ç»ƒ (æŽ¨èç”¨äºŽå¤§å¤šæ•°æƒ…å†µ)
# =============================================================================

echo "ðŸ“Š ç­–ç•¥1: åŸºç¡€å¢žå¼ºè®­ç»ƒ"

python vulnerability_detection_enhanced.py \
    --train_data_file=${DATA_DIR}/train.jsonl \
    --eval_data_file=${DATA_DIR}/valid.jsonl \
    --test_data_file=${DATA_DIR}/test.jsonl \
    --output_dir=${OUTPUT_DIR}/basic_enhanced \
    --model_type=roberta \
    --tokenizer_name=microsoft/codebert-base \
    --model_name_or_path=${PRETRAIN_DIR}/${MODEL_TYPE} \
    --num_train_epochs=6 \
    --block_size=512 \
    --train_batch_size=32 \
    --eval_batch_size=64 \
    --learning_rate=1e-5 \
    --max_grad_norm=1.0 \
    --seed=42 \
    --dropout_rate=0.3 \
    --pooling_strategy=attention \
    --use_focal_loss \
    --focal_alpha=2.0 \
    --focal_gamma=2.0 \
    --use_early_stopping \
    --patience=3 \
    --use_gradual_unfreezing \
    --use_weighted_sampling \
    2>&1 | tee ${OUTPUT_DIR}/basic_enhanced/train.log

# =============================================================================
# ç­–ç•¥2: æ•°æ®å¢žå¼º + å¯¹æ¯”å­¦ä¹  (è¿½æ±‚æœ€ä½³æ€§èƒ½)
# =============================================================================

echo "ðŸ”¥ ç­–ç•¥2: æ•°æ®å¢žå¼º + å¯¹æ¯”å­¦ä¹ "

python vulnerability_detection_enhanced.py \
    --train_data_file=${DATA_DIR}/train.jsonl \
    --eval_data_file=${DATA_DIR}/valid.jsonl \
    --test_data_file=${DATA_DIR}/test.jsonl \
    --output_dir=${OUTPUT_DIR}/contrastive_augmented \
    --model_type=roberta \
    --tokenizer_name=microsoft/codebert-base \
    --model_name_or_path=${PRETRAIN_DIR}/${MODEL_TYPE} \
    --num_train_epochs=6 \
    --block_size=512 \
    --train_batch_size=32 \
    --eval_batch_size=64 \
    --learning_rate=8e-6 \
    --max_grad_norm=1.0 \
    --seed=42 \
    --use_data_augmentation \
    --augmentation_ratio=0.3 \
    --dropout_rate=0.2 \
    --pooling_strategy=attention \
    --use_focal_loss \
    --focal_alpha=3.0 \
    --focal_gamma=2.5 \
    --label_smoothing=0.1 \
    --use_contrastive \
    --contrastive_temperature=0.07 \
    --contrastive_weight=0.2 \
    --use_early_stopping \
    --patience=4 \
    --use_gradual_unfreezing \
    --use_weighted_sampling \
    2>&1 | tee ${OUTPUT_DIR}/contrastive_augmented/train.log

# =============================================================================
# ç­–ç•¥3: è½»é‡åŒ–å¿«é€Ÿè®­ç»ƒ (é€‚åˆèµ„æºå—é™çŽ¯å¢ƒ)
# =============================================================================

echo "âš¡ ç­–ç•¥3: è½»é‡åŒ–å¿«é€Ÿè®­ç»ƒ"

python vulnerability_detection_enhanced.py \
    --train_data_file=${DATA_DIR}/train.jsonl \
    --eval_data_file=${DATA_DIR}/valid.jsonl \
    --test_data_file=${DATA_DIR}/test.jsonl \
    --output_dir=${OUTPUT_DIR}/lightweight \
    --model_type=roberta \
    --tokenizer_name=microsoft/codebert-base \
    --model_name_or_path=${PRETRAIN_DIR}/${MODEL_TYPE} \
    --num_train_epochs=4 \
    --block_size=256 \
    --train_batch_size=48 \
    --eval_batch_size=96 \
    --learning_rate=2e-5 \
    --max_grad_norm=1.0 \
    --seed=42 \
    --dropout_rate=0.1 \
    --pooling_strategy=cls \
    --use_early_stopping \
    --patience=2 \
    --use_weighted_sampling \
    2>&1 | tee ${OUTPUT_DIR}/lightweight/train.log

# =============================================================================
# ç­–ç•¥4: é›†æˆå­¦ä¹  (è®­ç»ƒå¤šä¸ªæ¨¡åž‹è¿›è¡ŒæŠ•ç¥¨)
# =============================================================================

echo "ðŸŽ¯ ç­–ç•¥4: é›†æˆå­¦ä¹ "

# è®­ç»ƒ3ä¸ªä¸åŒé…ç½®çš„æ¨¡åž‹
for i in {1..3}; do
    seed=$((42 + i * 100))
    lr=$(python -c "print(f'{1e-5 + i * 5e-6:.2e}')")
    
    echo "è®­ç»ƒé›†æˆæ¨¡åž‹ ${i}/3, seed=${seed}, lr=${lr}"
    
    python vulnerability_detection_enhanced.py \
        --train_data_file=${DATA_DIR}/train.jsonl \
        --eval_data_file=${DATA_DIR}/valid.jsonl \
        --test_data_file=${DATA_DIR}/test.jsonl \
        --output_dir=${OUTPUT_DIR}/ensemble/model_${i} \
        --model_type=roberta \
        --tokenizer_name=microsoft/codebert-base \
        --model_name_or_path=${PRETRAIN_DIR}/${MODEL_TYPE} \
        --num_train_epochs=6 \
        --block_size=512 \
        --train_batch_size=32 \
        --eval_batch_size=64 \
        --learning_rate=${lr} \
        --max_grad_norm=1.0 \
        --seed=${seed} \
        --dropout_rate=0.2 \
        --pooling_strategy=attention \
        --use_focal_loss \
        --focal_alpha=2.0 \
        --focal_gamma=2.0 \
        --use_early_stopping \
        --patience=3 \
        --use_gradual_unfreezing \
        --use_weighted_sampling \
        2>&1 | tee ${OUTPUT_DIR}/ensemble/model_${i}/train.log
done

# =============================================================================
# ç»“æžœåˆ†æž
# =============================================================================

echo "ðŸ“Š è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨åˆ†æžç»“æžœ..."

# åˆ›å»ºç»“æžœæ±‡æ€»è„šæœ¬
cat > ${OUTPUT_DIR}/analyze_results.py << 'EOF'
#!/usr/bin/env python3
import json
import os
import glob
from tabulate import tabulate

def analyze_results(base_dir):
    results = []
    
    # æŸ¥æ‰¾æ‰€æœ‰test_results.jsonæ–‡ä»¶
    result_files = glob.glob(f"{base_dir}/**/test_results.json", recursive=True)
    
    for file_path in result_files:
        try:
            with open(file_path, 'r') as f:
                metrics = json.load(f)
            
            strategy = os.path.dirname(file_path).split('/')[-1]
            
            results.append([
                strategy,
                f"{metrics['Acc']:.4f}",
                f"{metrics['Pos_f1']:.4f}",
                f"{metrics['PRC_AUC']:.4f}",
                f"{metrics['Pos_pre']:.4f}",
                f"{metrics['Pos_rec']:.4f}"
            ])
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    if results:
        headers = ["ç­–ç•¥", "å‡†ç¡®çŽ‡", "F1åˆ†æ•°", "AUC", "ç²¾ç¡®çŽ‡", "å¬å›žçŽ‡"]
        print("\nðŸ† å„ç­–ç•¥æ€§èƒ½å¯¹æ¯”:")
        print(tabulate(results, headers=headers, tablefmt="grid"))
        
        # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
        best_acc = max(results, key=lambda x: float(x[1]))
        best_f1 = max(results, key=lambda x: float(x[2]))
        best_auc = max(results, key=lambda x: float(x[3]))
        
        print(f"\nðŸ¥‡ æœ€ä½³å‡†ç¡®çŽ‡: {best_acc[0]} ({best_acc[1]})")
        print(f"ðŸ¥‡ æœ€ä½³F1åˆ†æ•°: {best_f1[0]} ({best_f1[2]})")
        print(f"ðŸ¥‡ æœ€ä½³AUC: {best_auc[0]} ({best_auc[3]})")
    else:
        print("æœªæ‰¾åˆ°æµ‹è¯•ç»“æžœæ–‡ä»¶")

if __name__ == "__main__":
    import sys
    base_dir = sys.argv[1] if len(sys.argv) > 1 else "."
    analyze_results(base_dir)
EOF

# è¿è¡Œç»“æžœåˆ†æž
python ${OUTPUT_DIR}/analyze_results.py ${OUTPUT_DIR}

# =============================================================================
# æ¨¡åž‹èžåˆ (å¯é€‰)
# =============================================================================

echo "ðŸ”„ æ¨¡åž‹èžåˆé¢„æµ‹"

cat > ${OUTPUT_DIR}/ensemble_predict.py << 'EOF'
#!/usr/bin/env python3
import torch
import numpy as np
import json
import glob
from torch.utils.data import DataLoader, SequentialSampler
from vulnerability_detection import TextDataset, evaluate_predictions
from vulmodel_enhanced import EnhancedVulModel
from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer

def ensemble_predict(model_dirs, test_file, tokenizer_name):
    # åŠ è½½tokenizer
    tokenizer = RobertaTokenizer.from_pretrained(tokenizer_name)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    class Args:
        block_size = 512
    
    test_dataset = TextDataset(tokenizer, Args(), test_file)
    test_dataloader = DataLoader(
        test_dataset,
        sampler=SequentialSampler(test_dataset),
        batch_size=32
    )
    
    all_predictions = []
    
    for model_dir in model_dirs:
        print(f"åŠ è½½æ¨¡åž‹: {model_dir}")
        
        # åŠ è½½æ¨¡åž‹
        config = RobertaConfig.from_pretrained("microsoft/codebert-base")
        config.num_labels = 1
        encoder = RobertaForSequenceClassification.from_pretrained(
            "microsoft/codebert-base", config=config
        )
        model = EnhancedVulModel(encoder, config, tokenizer, Args())
        
        # æŸ¥æ‰¾æœ€ä½³æ¨¡åž‹æ–‡ä»¶
        model_files = glob.glob(f"{model_dir}/best_model_*.bin")
        if model_files:
            model.load_state_dict(torch.load(model_files[0], map_location='cpu'))
            model.eval()
            
            predictions = []
            with torch.no_grad():
                for batch in test_dataloader:
                    inputs = batch[0]
                    probs = model(inputs)
                    predictions.append(probs.numpy())
            
            all_predictions.append(np.concatenate(predictions, axis=0))
    
    if all_predictions:
        # å¹³å‡é¢„æµ‹
        ensemble_preds = np.mean(all_predictions, axis=0)
        
        # è¯„ä¼°
        test_labels = np.array([example.label for example in test_dataset.examples])
        metrics = evaluate_predictions(test_labels, ensemble_preds)
        
        print("ðŸŽ¯ é›†æˆæ¨¡åž‹ç»“æžœ:")
        for key, value in metrics.items():
            print(f"  {key}: {value:.4f}")
        
        return metrics
    else:
        print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡åž‹æ–‡ä»¶")
        return None

if __name__ == "__main__":
    model_dirs = glob.glob("ensemble/model_*")
    if model_dirs:
        ensemble_predict(model_dirs, "../../data/finetune_data/c_vulnerability/devign/test.jsonl", "microsoft/codebert-base")
    else:
        print("æœªæ‰¾åˆ°é›†æˆæ¨¡åž‹ç›®å½•")
EOF

# å¦‚æžœæœ‰é›†æˆæ¨¡åž‹ï¼Œè¿è¡Œèžåˆé¢„æµ‹
if [ -d "${OUTPUT_DIR}/ensemble" ]; then
    cd ${OUTPUT_DIR}
    python ensemble_predict.py
    cd -
fi

echo "âœ… æ‰€æœ‰è®­ç»ƒç­–ç•¥æ‰§è¡Œå®Œæˆï¼"
echo "ðŸ“Š è¯¦ç»†ç»“æžœè¯·æŸ¥çœ‹: ${OUTPUT_DIR}"
echo ""
echo "ðŸ’¡ ä½¿ç”¨å»ºè®®:"
echo "   - ç­–ç•¥1: é€‚åˆå¤§å¤šæ•°æƒ…å†µï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆçŽ‡"
echo "   - ç­–ç•¥2: è¿½æ±‚æœ€ä½³æ€§èƒ½ï¼Œéœ€è¦æ›´å¤šè®¡ç®—èµ„æº"
echo "   - ç­–ç•¥3: å¿«é€ŸéªŒè¯ï¼Œé€‚åˆåŽŸåž‹å¼€å‘"
echo "   - ç­–ç•¥4: æœ€é«˜æ€§èƒ½ï¼Œé€šè¿‡é›†æˆå¤šä¸ªæ¨¡åž‹" 