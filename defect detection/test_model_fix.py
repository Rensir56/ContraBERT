#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer
from vulmodel_enhanced import EnhancedVulModel, VulModelWithContrastive

def test_model():
    """æµ‹è¯•ä¿®å¤åçš„æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸è¿è¡Œ"""
    
    # æ¨¡æ‹Ÿå‚æ•°
    class Args:
        dropout_rate = 0.3
        num_attention_heads = 8
        attention_dropout = 0.1
        pooling_strategy = 'attention'
        use_focal_loss = True
        focal_alpha = 2.0
        focal_gamma = 2.0
        use_contrastive = False
    
    args = Args()
    
    # åˆå§‹åŒ–tokenizerå’Œé…ç½®
    tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')
    config = RobertaConfig.from_pretrained('microsoft/codebert-base')
    config.num_labels = 1
    
    # åˆå§‹åŒ–ç¼–ç å™¨
    encoder = RobertaForSequenceClassification.from_pretrained(
        'microsoft/codebert-base',
        config=config
    )
    
    # æµ‹è¯•EnhancedVulModel
    print("ğŸ§ª æµ‹è¯• EnhancedVulModel...")
    model = EnhancedVulModel(encoder, config, tokenizer, args)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 4
    seq_len = 128
    input_ids = torch.randint(0, tokenizer.vocab_size, (batch_size, seq_len))
    labels = torch.randint(0, 2, (batch_size,)).float()
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    try:
        loss, probs = model(input_ids, labels)
        print(f"âœ… EnhancedVulModel æµ‹è¯•é€šè¿‡!")
        print(f"   Loss shape: {loss.shape}")
        print(f"   Probs shape: {probs.shape}")
        print(f"   Loss value: {loss.item():.4f}")
    except Exception as e:
        print(f"âŒ EnhancedVulModel æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•ä¸å¸¦æ ‡ç­¾çš„æ¨ç†
    try:
        probs = model(input_ids)
        print(f"âœ… æ¨ç†æ¨¡å¼æµ‹è¯•é€šè¿‡!")
        print(f"   Probs shape: {probs.shape}")
    except Exception as e:
        print(f"âŒ æ¨ç†æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•å¯¹æ¯”å­¦ä¹ æ¨¡å‹
    print("\nğŸ§ª æµ‹è¯• VulModelWithContrastive...")
    args.use_contrastive = True
    contrastive_model = VulModelWithContrastive(encoder, config, tokenizer, args)
    
    try:
        contrastive_model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ä»¥å¯ç”¨å¯¹æ¯”å­¦ä¹ 
        loss, probs = contrastive_model(input_ids, labels)
        print(f"âœ… VulModelWithContrastive æµ‹è¯•é€šè¿‡!")
        print(f"   Loss shape: {loss.shape}")
        print(f"   Probs shape: {probs.shape}")
        print(f"   Loss value: {loss.item():.4f}")
    except Exception as e:
        print(f"âŒ VulModelWithContrastive æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†!")
    return True

if __name__ == "__main__":
    test_model() 