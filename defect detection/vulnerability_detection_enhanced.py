#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import os
import random
import numpy as np
import torch
import json
import logging
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from transformers import (
    AdamW, 
    RobertaConfig, 
    RobertaForSequenceClassification, 
    RobertaTokenizer,
    get_cosine_schedule_with_warmup
)
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

# 导入自定义模块
from vulnerability_detection import TextDataset, set_seed, evaluate_predictions
from vulmodel_enhanced import EnhancedVulModel, VulModelWithContrastive
from training_strategies import (
    EarlyStopping, 
    GradualUnfreezing, 
    FocalLossWithLabelSmoothing,
    get_optimizer_with_different_lr,
    WeightedSampler
)
from data_augmentation import create_augmented_dataset

# 设置日志
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    datefmt='%m/%d/%Y %H:%M:%S',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

class ModelConfig:
    """模型配置类"""
    def __init__(self, args):
        # 基础配置
        self.model_type = args.model_type
        self.model_name_or_path = args.model_name_or_path
        self.block_size = args.block_size
        
        # 增强配置
        self.dropout_rate = getattr(args, 'dropout_rate', 0.3)
        self.num_attention_heads = getattr(args, 'num_attention_heads', 8)
        self.attention_dropout = getattr(args, 'attention_dropout', 0.1)
        self.pooling_strategy = getattr(args, 'pooling_strategy', 'attention')
        
        # 损失函数配置
        self.use_focal_loss = getattr(args, 'use_focal_loss', True)
        self.focal_alpha = getattr(args, 'focal_alpha', 2.0)
        self.focal_gamma = getattr(args, 'focal_gamma', 2.0)
        self.label_smoothing = getattr(args, 'label_smoothing', 0.1)
        
        # 对比学习配置
        self.use_contrastive = getattr(args, 'use_contrastive', False)
        self.contrastive_temperature = getattr(args, 'contrastive_temperature', 0.1)
        self.contrastive_weight = getattr(args, 'contrastive_weight', 0.1)

class EnhancedTrainer:
    """增强训练器"""
    
    def __init__(self, args, model, tokenizer, train_dataset, eval_dataset):
        self.args = args
        self.model = model
        self.tokenizer = tokenizer
        self.train_dataset = train_dataset
        self.eval_dataset = eval_dataset
        
        # 训练组件
        self.optimizer = None
        self.scheduler = None
        self.early_stopping = None
        self.gradual_unfreezing = None
        
        # 设置设备
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        
        # 初始化训练组件
        self._setup_training_components()
    
    def _setup_training_components(self):
        """设置训练组件"""
        # 优化器 - 不同层使用不同学习率
        self.optimizer = get_optimizer_with_different_lr(
            self.model, 
            base_lr=self.args.learning_rate,
            classifier_lr=self.args.learning_rate * 2
        )
        
        # 数据加载器
        self._setup_dataloaders()
        
        # 学习率调度器
        num_training_steps = len(self.train_dataloader) * self.args.num_train_epochs
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=int(0.1 * num_training_steps),
            num_training_steps=num_training_steps
        )
        
        # 早停
        if getattr(self.args, 'use_early_stopping', True):
            self.early_stopping = EarlyStopping(
                patience=getattr(self.args, 'patience', 3),
                min_delta=0.001
            )
        
        # 渐进式解冻
        if getattr(self.args, 'use_gradual_unfreezing', True):
            unfreeze_schedule = [1, 2, 3]  # 在第1,2,3个epoch解冻更多层
            self.gradual_unfreezing = GradualUnfreezing(
                self.model, unfreeze_schedule
            )
    
    def _setup_dataloaders(self):
        """设置数据加载器"""
        # 训练数据加载器 - 使用加权采样解决类别不平衡
        if getattr(self.args, 'use_weighted_sampling', True):
            labels = [example.label for example in self.train_dataset.examples]
            weighted_sampler = WeightedSampler(labels)
            self.train_dataloader = DataLoader(
                self.train_dataset,
                sampler=weighted_sampler.get_sampler(),
                batch_size=self.args.train_batch_size,
                num_workers=4,
                pin_memory=True
            )
        else:
            self.train_dataloader = DataLoader(
                self.train_dataset,
                sampler=RandomSampler(self.train_dataset),
                batch_size=self.args.train_batch_size,
                num_workers=4,
                pin_memory=True
            )
        
        # 验证数据加载器
        self.eval_dataloader = DataLoader(
            self.eval_dataset,
            sampler=SequentialSampler(self.eval_dataset),
            batch_size=self.args.eval_batch_size,
            num_workers=4,
            pin_memory=True
        )
    
    def train_epoch(self, epoch):
        """训练一个epoch"""
        self.model.train()
        
        # 渐进式解冻
        if self.gradual_unfreezing:
            self.gradual_unfreezing.step(epoch)
        
        total_loss = 0
        num_steps = 0
        
        progress_bar = tqdm(
            self.train_dataloader,
            desc=f"Epoch {epoch+1}/{self.args.num_train_epochs}"
        )
        
        for step, batch in enumerate(progress_bar):
            inputs = batch[0].to(self.device)
            labels = batch[1].to(self.device)
            
            self.optimizer.zero_grad()
            
            # 前向传播
            loss, logits = self.model(inputs, labels)
            
            # 反向传播
            loss.backward()
            
            # 梯度裁剪
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.args.max_grad_norm
            )
            
            self.optimizer.step()
            self.scheduler.step()
            
            total_loss += loss.item()
            num_steps += 1
            
            # 更新进度条
            current_lr = self.scheduler.get_last_lr()[0]
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{total_loss/num_steps:.4f}',
                'lr': f'{current_lr:.2e}'
            })
        
        return total_loss / num_steps
    
    def evaluate(self):
        """评估模型"""
        self.model.eval()
        total_loss = 0
        all_logits = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(self.eval_dataloader, desc="Evaluating"):
                inputs = batch[0].to(self.device)
                labels = batch[1].to(self.device)
                
                loss, logits = self.model(inputs, labels)
                
                total_loss += loss.item()
                all_logits.append(logits.cpu().numpy())
                all_labels.append(labels.cpu().numpy())
        
        # 计算指标
        all_logits = np.concatenate(all_logits, axis=0)
        all_labels = np.concatenate(all_labels, axis=0)
        
        metrics = evaluate_predictions(all_labels, all_logits)
        metrics['eval_loss'] = total_loss / len(self.eval_dataloader)
        
        return metrics
    
    def train(self):
        """主训练循环"""
        logger.info("***** 开始训练 *****")
        logger.info(f"  样本数量 = {len(self.train_dataset)}")
        logger.info(f"  Epochs = {self.args.num_train_epochs}")
        logger.info(f"  批次大小 = {self.args.train_batch_size}")
        
        best_acc = 0.0
        best_model_path = None
        
        for epoch in range(self.args.num_train_epochs):
            # 训练一个epoch
            train_loss = self.train_epoch(epoch)
            
            # 评估
            eval_metrics = self.evaluate()
            
            logger.info(f"Epoch {epoch+1}:")
            logger.info(f"  训练损失: {train_loss:.4f}")
            logger.info(f"  验证损失: {eval_metrics['eval_loss']:.4f}")
            logger.info(f"  准确率: {eval_metrics['Acc']:.4f}")
            logger.info(f"  F1分数: {eval_metrics['Pos_f1']:.4f}")
            logger.info(f"  AUC: {eval_metrics['PRC_AUC']:.4f}")
            
            # 保存最佳模型
            current_acc = eval_metrics['Acc']
            if current_acc > best_acc:
                best_acc = current_acc
                best_model_path = os.path.join(
                    self.args.output_dir, 
                    f'best_model_epoch_{epoch+1}_acc_{best_acc:.4f}.bin'
                )
                torch.save(self.model.state_dict(), best_model_path)
                logger.info(f"保存最佳模型: {best_model_path}")
            
            # 早停检查
            if self.early_stopping:
                if self.early_stopping(eval_metrics['eval_loss'], self.model):
                    logger.info(f"早停在第 {epoch+1} 个epoch")
                    break
        
        logger.info(f"训练完成！最佳准确率: {best_acc:.4f}")
        return best_model_path

def parse_args():
    """解析命令行参数"""
    parser = argparse.ArgumentParser()
    
    # 基础参数
    parser.add_argument("--train_data_file", required=True, type=str)
    parser.add_argument("--eval_data_file", required=True, type=str)
    parser.add_argument("--test_data_file", type=str)
    parser.add_argument("--output_dir", required=True, type=str)
    
    # 模型参数
    parser.add_argument("--model_type", default="roberta", type=str)
    parser.add_argument("--model_name_or_path", required=True, type=str)
    parser.add_argument("--tokenizer_name", type=str)
    parser.add_argument("--block_size", default=512, type=int)
    
    # 训练参数
    parser.add_argument("--num_train_epochs", default=5, type=int)
    parser.add_argument("--train_batch_size", default=16, type=int)
    parser.add_argument("--eval_batch_size", default=32, type=int)
    parser.add_argument("--learning_rate", default=2e-5, type=float)
    parser.add_argument("--max_grad_norm", default=1.0, type=float)
    parser.add_argument("--seed", default=42, type=int)
    
    # 增强参数
    parser.add_argument("--use_data_augmentation", action='store_true')
    parser.add_argument("--augmentation_ratio", default=0.3, type=float)
    parser.add_argument("--dropout_rate", default=0.3, type=float)
    parser.add_argument("--pooling_strategy", default="attention", 
                       choices=["cls", "mean", "max", "attention"])
    
    # 损失函数参数
    parser.add_argument("--use_focal_loss", action='store_true')
    parser.add_argument("--focal_alpha", default=2.0, type=float)
    parser.add_argument("--focal_gamma", default=2.0, type=float)
    parser.add_argument("--label_smoothing", default=0.1, type=float)
    
    # 对比学习参数
    parser.add_argument("--use_contrastive", action='store_true')
    parser.add_argument("--contrastive_temperature", default=0.1, type=float)
    parser.add_argument("--contrastive_weight", default=0.1, type=float)
    
    # 训练策略参数
    parser.add_argument("--use_early_stopping", action='store_true')
    parser.add_argument("--patience", default=3, type=int)
    parser.add_argument("--use_gradual_unfreezing", action='store_true')
    parser.add_argument("--use_weighted_sampling", action='store_true')
    
    return parser.parse_args()

def main():
    args = parse_args()
    
    # 设置随机种子
    set_seed(args.seed)
    
    # 创建输出目录
    os.makedirs(args.output_dir, exist_ok=True)
    
    # 数据增强
    if args.use_data_augmentation:
        augmented_file = args.train_data_file.replace('.jsonl', '_augmented.jsonl')
        if not os.path.exists(augmented_file):
            logger.info("正在进行数据增强...")
            create_augmented_dataset(
                args.train_data_file,
                augmented_file,
                args.augmentation_ratio
            )
        args.train_data_file = augmented_file
    
    # 加载tokenizer
    tokenizer_name = args.tokenizer_name or args.model_name_or_path
    tokenizer = RobertaTokenizer.from_pretrained(tokenizer_name)
    
    # 加载数据集
    logger.info("加载数据集...")
    train_dataset = TextDataset(tokenizer, args, args.train_data_file)
    eval_dataset = TextDataset(tokenizer, args, args.eval_data_file)
    
    # 加载模型
    logger.info("加载模型...")
    config = RobertaConfig.from_pretrained(args.model_name_or_path)
    config.num_labels = 1
    
    encoder = RobertaForSequenceClassification.from_pretrained(
        args.model_name_or_path,
        config=config
    )
    
    # 选择模型类型
    if args.use_contrastive:
        model = VulModelWithContrastive(encoder, config, tokenizer, args)
        logger.info("使用对比学习增强模型")
    else:
        model = EnhancedVulModel(encoder, config, tokenizer, args)
        logger.info("使用增强模型")
    
    # 创建训练器
    trainer = EnhancedTrainer(args, model, tokenizer, train_dataset, eval_dataset)
    
    # 开始训练
    best_model_path = trainer.train()
    
    # 测试（如果提供了测试数据）
    if args.test_data_file:
        logger.info("开始测试...")
        model.load_state_dict(torch.load(best_model_path))
        test_dataset = TextDataset(tokenizer, args, args.test_data_file)
        test_dataloader = DataLoader(
            test_dataset,
            sampler=SequentialSampler(test_dataset),
            batch_size=args.eval_batch_size
        )
        
        model.eval()
        test_logits = []
        test_labels = []
        
        with torch.no_grad():
            for batch in tqdm(test_dataloader, desc="Testing"):
                inputs = batch[0].to(trainer.device)
                labels = batch[1].to(trainer.device)
                
                probs = model(inputs)
                test_logits.append(probs.cpu().numpy())
                test_labels.append(labels.cpu().numpy())
        
        test_logits = np.concatenate(test_logits, axis=0)
        test_labels = np.concatenate(test_labels, axis=0)
        
        test_metrics = evaluate_predictions(test_labels, test_logits)
        
        logger.info("测试结果:")
        for key, value in test_metrics.items():
            logger.info(f"  {key}: {value:.4f}")
        
        # 保存测试结果
        with open(os.path.join(args.output_dir, 'test_results.json'), 'w') as f:
            json.dump(test_metrics, f, indent=2)

if __name__ == "__main__":
    main()