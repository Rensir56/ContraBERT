# coding=utf-8
"""
ContraBERT 漏洞检测优化版本 V3
整合多种SOTA技术，专门优化代码漏洞检测任务
"""

from __future__ import absolute_import, division, print_function

import argparse
import logging
import os
import pickle
import random
import json
import gzip
import math
from collections import Counter
from sklearn.metrics import confusion_matrix, classification_report, average_precision_score, accuracy_score
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler, WeightedRandomSampler
from torch.utils.data.distributed import DistributedSampler
from tqdm import tqdm, trange
import multiprocessing
from vulmodel import VulModel
from transformers import (
    AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup,
    BertConfig, BertForMaskedLM, BertTokenizer,
    RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,
    get_polynomial_decay_schedule_with_warmup
)

logger = logging.getLogger(__name__)

MODEL_CLASSES = {
    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),
}

class FocalLoss(nn.Module):
    """Focal Loss for addressing class imbalance"""
    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs, targets):
        ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class LabelSmoothingLoss(nn.Module):
    """Label smoothing for better generalization"""
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
        
    def forward(self, pred, target):
        confidence = 1. - self.smoothing
        smooth_target = target * confidence + (1 - target) * self.smoothing
        return F.binary_cross_entropy_with_logits(pred, smooth_target)

class MultiHeadAttentionPooling(nn.Module):
    """Multi-head attention pooling for better representation"""
    def __init__(self, hidden_size, num_heads=8):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        self.query = nn.Linear(hidden_size, hidden_size)
        self.key = nn.Linear(hidden_size, hidden_size)
        self.value = nn.Linear(hidden_size, hidden_size)
        self.out_proj = nn.Linear(hidden_size, hidden_size)
        
    def forward(self, hidden_states, attention_mask=None):
        batch_size, seq_len, hidden_size = hidden_states.shape
        
        Q = self.query(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        if attention_mask is not None:
            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            scores = scores.masked_fill(attention_mask == 0, -1e9)
            
        attn_weights = F.softmax(scores, dim=-1)
        context = torch.matmul(attn_weights, V)
        
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_size)
        output = self.out_proj(context)
        
        # Global pooling with attention
        pooled = torch.sum(output * attn_weights.mean(dim=1).mean(dim=1).unsqueeze(-1), dim=1)
        return pooled

class EnhancedVulModel(nn.Module):
    """Enhanced vulnerability detection model with multiple optimizations"""
    def __init__(self, encoder, config, tokenizer, args):
        super().__init__()
        self.encoder = encoder
        self.config = config
        self.tokenizer = tokenizer
        self.args = args
        
        # Enhanced pooling strategies
        self.pooling_strategy = getattr(args, 'pooling_strategy', 'attention')
        
        if self.pooling_strategy == 'attention':
            self.attention_pooling = MultiHeadAttentionPooling(config.hidden_size)
        elif self.pooling_strategy == 'weighted':
            self.weighted_pooling = nn.Linear(config.hidden_size, 1)
            
        # Multi-layer classifier with dropout
        classifier_layers = []
        hidden_sizes = [config.hidden_size, config.hidden_size // 2, config.num_labels]
        dropout_rate = getattr(args, 'dropout_rate', 0.3)
        
        for i in range(len(hidden_sizes) - 1):
            classifier_layers.extend([
                nn.Linear(hidden_sizes[i], hidden_sizes[i+1]),
                nn.LayerNorm(hidden_sizes[i+1]) if i < len(hidden_sizes) - 2 else nn.Identity(),
                nn.ReLU() if i < len(hidden_sizes) - 2 else nn.Identity(),
                nn.Dropout(dropout_rate) if i < len(hidden_sizes) - 2 else nn.Identity()
            ])
        
        self.classifier = nn.Sequential(*classifier_layers)
        
        # Loss functions
        self.use_focal_loss = getattr(args, 'use_focal_loss', False)
        self.use_label_smoothing = getattr(args, 'use_label_smoothing', False)
        
        if self.use_focal_loss:
            self.focal_loss = FocalLoss(
                alpha=getattr(args, 'focal_alpha', 2.0),
                gamma=getattr(args, 'focal_gamma', 2.0)
            )
        
        if self.use_label_smoothing:
            self.label_smoothing_loss = LabelSmoothingLoss(
                smoothing=getattr(args, 'label_smoothing', 0.1)
            )
            
    def forward(self, input_ids=None, labels=None):
        # Get encoder outputs
        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)
        outputs = self.encoder.roberta(input_ids, attention_mask=attention_mask, output_hidden_states=True)
        
        # Enhanced pooling
        last_hidden_state = outputs.last_hidden_state
        
        if self.pooling_strategy == 'cls':
            pooled_output = last_hidden_state[:, 0]  # [CLS] token
        elif self.pooling_strategy == 'mean':
            pooled_output = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)
        elif self.pooling_strategy == 'max':
            pooled_output = torch.max(last_hidden_state + (1 - attention_mask.unsqueeze(-1)) * -1e9, dim=1)[0]
        elif self.pooling_strategy == 'attention':
            pooled_output = self.attention_pooling(last_hidden_state, attention_mask)
        elif self.pooling_strategy == 'weighted':
            weights = F.softmax(self.weighted_pooling(last_hidden_state).squeeze(-1) + (1 - attention_mask) * -1e9, dim=1)
            pooled_output = torch.sum(last_hidden_state * weights.unsqueeze(-1), dim=1)
        else:
            pooled_output = last_hidden_state[:, 0]
            
        # Classification
        logits = self.classifier(pooled_output)
        probs = torch.sigmoid(logits)
        
        if labels is not None:
            labels = labels.float().unsqueeze(-1)
            
            # Choose loss function
            if self.use_focal_loss:
                loss = self.focal_loss(logits, labels)
            elif self.use_label_smoothing:
                loss = self.label_smoothing_loss(logits, labels)
            else:
                loss = F.binary_cross_entropy_with_logits(logits, labels)
                
            return loss, probs
        else:
            return probs

class InputFeatures(object):
    """A single training/test features for a example."""
    def __init__(self, input_tokens, input_ids, idx, label):
        self.input_tokens = input_tokens
        self.input_ids = input_ids
        self.label = label
        self.idx = str(idx)

def convert_examples_to_features(js, tokenizer, args, isdevign):
    """Convert raw examples to features"""
    # source
    if isdevign:
        code = ' '.join(js['func'].split())
    else:
        if js['target'] == 1:
            code = ' '.join(js['pair']['vuln']['tokens'])
        else:
            code = ' '.join(js['pair']['patch']['tokens'])
    
    code_tokens = tokenizer.tokenize(code)[:args.block_size - 2]
    source_tokens = [tokenizer.bos_token] + code_tokens + [tokenizer.eos_token]
    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)
    padding_length = args.block_size - len(source_ids)
    source_ids += [tokenizer.pad_token_id] * padding_length
    return InputFeatures(source_tokens, source_ids, js['idx'], js['target'])

class TextDataset(Dataset):
    def __init__(self, tokenizer, args, file_path=None):
        self.examples = []
        logger.info(f"Loading dataset from {file_path}")
        
        if file_path.endswith('.gz'):
            isdevign = False
            with gzip.GzipFile(file_path, 'r') as f:
                lines = f.readlines()
        else:
            isdevign = True
            with open(file_path) as f:
                lines = f.readlines()
                
        logger.info(f"Processing {len(lines)} examples")
        
        for line in tqdm(lines, desc="Converting examples to features"):
            js = json.loads(line.strip())
            self.examples.append(convert_examples_to_features(js, tokenizer, args, isdevign))
        
        # Print class distribution
        labels = [example.label for example in self.examples]
        label_counts = Counter(labels)
        logger.info(f"Dataset loaded: {len(self.examples)} examples")
        logger.info(f"Class distribution: {dict(label_counts)}")
        
        if 'train' in file_path:
            for idx, example in enumerate(self.examples[:1]):
                logger.info("*** Example ***")
                logger.info("idx: {}".format(idx))
                logger.info("label: {}".format(example.label))
                logger.info("input_tokens: {}".format([x.replace('\u0120', '_') for x in example.input_tokens[:20]]))

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return torch.tensor(self.examples[i].input_ids), torch.tensor(self.examples[i].label)

def set_seed(seed=42):
    """Set random seeds for reproducibility"""
    random.seed(seed)
    os.environ['PYHTONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class AdvancedOptimizer:
    """Advanced optimizer with different learning rates for different parts"""
    def __init__(self, model, args):
        self.args = args
        
        # Different learning rates for different parts
        no_decay = ['bias', 'LayerNorm.weight']
        
        encoder_params = []
        classifier_params = []
        
        for name, param in model.named_parameters():
            if 'encoder' in name:
                encoder_params.append((name, param))
            else:
                classifier_params.append((name, param))
        
        optimizer_grouped_parameters = [
            # Encoder parameters with decay
            {'params': [p for n, p in encoder_params if not any(nd in n for nd in no_decay)],
             'weight_decay': args.weight_decay, 'lr': args.learning_rate * 0.5},
            # Encoder parameters without decay  
            {'params': [p for n, p in encoder_params if any(nd in n for nd in no_decay)],
             'weight_decay': 0.0, 'lr': args.learning_rate * 0.5},
            # Classifier parameters with decay
            {'params': [p for n, p in classifier_params if not any(nd in n for nd in no_decay)],
             'weight_decay': args.weight_decay, 'lr': args.learning_rate},
            # Classifier parameters without decay
            {'params': [p for n, p in classifier_params if any(nd in n for nd in no_decay)],
             'weight_decay': 0.0, 'lr': args.learning_rate}
        ]
        
        self.optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
    
    def get_optimizer(self):
        return self.optimizer

class EarlyStopping:
    """Early stopping to prevent overfitting"""
    def __init__(self, patience=3, min_delta=0.001, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = float('inf')
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best_weights:
                self.best_weights = model.state_dict().copy()
            return False
        else:
            self.counter += 1
            if self.counter >= self.patience:
                if self.restore_best_weights and self.best_weights:
                    model.load_state_dict(self.best_weights)
                return True
        return False

def create_weighted_sampler(dataset, args):
    """Create weighted sampler for handling class imbalance with enhanced strategy"""
    if not getattr(args, 'use_weighted_sampling', False):
        return RandomSampler(dataset)
    
    labels = [example.label for example in dataset.examples]
    class_counts = Counter(labels)
    
    # More sophisticated weighting
    total_samples = len(labels)
    weights = {}
    for label, count in class_counts.items():
        # Inverse frequency with square root to avoid extreme weights
        weights[label] = math.sqrt(total_samples / count)
    
    sample_weights = [weights[label] for label in labels]
    
    logger.info(f"Class weights: {weights}")
    logger.info(f"Sample weights range: {min(sample_weights):.3f} - {max(sample_weights):.3f}")
    
    return WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )

def train(args, train_dataset, model, tokenizer):
    """Enhanced training function with multiple optimizations"""
    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
    
    # Enhanced sampling strategy
    if args.local_rank == -1:
        train_sampler = create_weighted_sampler(train_dataset, args)
    else:
        train_sampler = DistributedSampler(train_dataset)

    train_dataloader = DataLoader(
        train_dataset, 
        sampler=train_sampler,
        batch_size=args.train_batch_size, 
        num_workers=4, 
        pin_memory=True
    )
    
    # Training parameters
    args.max_steps = args.epoch * len(train_dataloader)
    args.save_steps = len(train_dataloader)
    args.warmup_steps = int(0.1 * args.max_steps)  # 10% warmup
    args.logging_steps = len(train_dataloader)
    args.num_train_epochs = args.epoch
    
    model.to(args.device)
    
    # Advanced optimizer
    advanced_optimizer = AdvancedOptimizer(model, args)
    optimizer = advanced_optimizer.get_optimizer()
    
    # Enhanced scheduler with warmup and polynomial decay
    scheduler = get_polynomial_decay_schedule_with_warmup(
        optimizer,
        num_warmup_steps=args.warmup_steps,
        num_training_steps=args.max_steps,
        power=2.0
    )
    
    # Early stopping
    early_stopping = EarlyStopping(
        patience=getattr(args, 'patience', 3),
        min_delta=0.001
    ) if getattr(args, 'use_early_stopping', False) else None
    
    # Multi-GPU setup
    if args.n_gpu > 1:
        model = torch.nn.DataParallel(model)

    if args.local_rank != -1:
        model = torch.nn.parallel.DistributedDataParallel(
            model, 
            device_ids=[args.local_rank],
            output_device=args.local_rank,
            find_unused_parameters=True
        )

    # Training info
    logger.info("***** Running training *****")
    logger.info("  Num examples = %d", len(train_dataset))
    logger.info("  Num Epochs = %d", args.num_train_epochs)
    logger.info("  Instantaneous batch size per GPU = %d", args.per_gpu_train_batch_size)
    logger.info("  Total train batch size = %d", args.train_batch_size)
    logger.info("  Gradient Accumulation steps = %d", args.gradient_accumulation_steps)
    logger.info("  Total optimization steps = %d", args.max_steps)
    logger.info("  Warmup steps = %d", args.warmup_steps)

    global_step = 0
    tr_loss, logging_loss, avg_loss, tr_nb, tr_num, train_loss = 0.0, 0.0, 0.0, 0, 0, 0
    best_acc = 0.0
    model.zero_grad()

    epoch_iterator = trange(0, int(args.num_train_epochs), desc="Epoch", disable=args.local_rank not in [-1, 0])
    
    for epoch in epoch_iterator:
        epoch_loss = 0.0
        epoch_steps = 0
        
        batch_iterator = tqdm(
            train_dataloader, 
            desc=f"Training Epoch {epoch+1}/{int(args.num_train_epochs)}", 
            disable=args.local_rank not in [-1, 0]
        )
        
        for step, batch in enumerate(batch_iterator):
            model.train()
            inputs = batch[0].to(args.device)
            labels = batch[1].to(args.device)
            
            loss, logits = model(inputs, labels)

            if args.n_gpu > 1:
                loss = loss.mean()
            if args.gradient_accumulation_steps > 1:
                loss = loss / args.gradient_accumulation_steps

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

            tr_loss += loss.item()
            epoch_loss += loss.item()
            epoch_steps += 1
            
            if (step + 1) % args.gradient_accumulation_steps == 0:
                optimizer.step()
                scheduler.step()
                model.zero_grad()
                global_step += 1
                
                # Update progress
                current_lr = scheduler.get_last_lr()[0]
                avg_epoch_loss = epoch_loss / epoch_steps
                
                batch_iterator.set_postfix({
                    'loss': f'{avg_epoch_loss:.4f}',
                    'lr': f'{current_lr:.2e}',
                    'step': f'{global_step}/{args.max_steps}'
                })

        # Evaluation at end of each epoch
        if args.local_rank in [-1, 0] and args.evaluate_during_training:
            results = evaluate(args, model, tokenizer, eval_when_training=True)
            
            for key, value in results.items():
                logger.info(f"Epoch {epoch+1} - {key}: {round(value, 4)}")

            # Early stopping check
            if early_stopping and early_stopping(results['eval_loss'], model):
                logger.info("Early stopping triggered!")
                break

            # Save best model
            if results['eval_acc'] > best_acc:
                best_acc = results['eval_acc']
                logger.info("  " + "*" * 20)
                logger.info(f"  New Best Accuracy: {round(best_acc, 4)}")
                logger.info("  " + "*" * 20)

                checkpoint_prefix = 'checkpoint-best-acc'
                output_dir = os.path.join(args.output_dir, checkpoint_prefix)
                os.makedirs(output_dir, exist_ok=True)
                
                model_to_save = model.module if hasattr(model, 'module') else model
                output_file = os.path.join(output_dir, 'model.bin')
                torch.save(model_to_save.state_dict(), output_file)
                logger.info(f"Saving model checkpoint to {output_file}")
                
        # Update epoch progress
        epoch_iterator.set_postfix({
            'avg_loss': f'{avg_epoch_loss:.4f}',
            'best_acc': f'{best_acc:.4f}'
        })

def evaluate(args, model, tokenizer, eval_when_training=False):
    """Enhanced evaluation function"""
    eval_output_dir = args.output_dir
    eval_dataset = TextDataset(tokenizer, args, args.eval_data_file)

    os.makedirs(eval_output_dir, exist_ok=True)

    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)
    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)
    eval_dataloader = DataLoader(
        eval_dataset, 
        sampler=eval_sampler, 
        batch_size=args.eval_batch_size, 
        num_workers=4,
        pin_memory=True
    )

    if args.n_gpu > 1 and eval_when_training is False:
        model = torch.nn.DataParallel(model)

    logger.info("***** Running evaluation *****")
    logger.info("  Num examples = %d", len(eval_dataset))
    logger.info("  Batch size = %d", args.eval_batch_size)
    
    eval_loss = 0.0
    nb_eval_steps = 0
    model.eval()
    logits = []
    labels = []
    
    eval_bar = tqdm(eval_dataloader, desc="Evaluating", disable=args.local_rank not in [-1, 0])
    
    for batch in eval_bar:
        inputs = batch[0].to(args.device)
        label = batch[1].to(args.device)
        
        with torch.no_grad():
            lm_loss, logit = model(inputs, label)
            eval_loss += lm_loss.mean().item()
            logits.append(logit.cpu().numpy())
            labels.append(label.cpu().numpy())
            
        nb_eval_steps += 1
        current_loss = eval_loss / nb_eval_steps
        eval_bar.set_postfix({'eval_loss': f'{current_loss:.4f}'})
        
    logits = np.concatenate(logits, 0)
    labels = np.concatenate(labels, 0)
    preds = (logits[:, 0] > 0.5).astype(int)
    
    # Enhanced metrics
    eval_acc = accuracy_score(labels, preds)
    eval_loss = eval_loss / nb_eval_steps
    
    # Additional metrics
    tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    result = {
        "eval_loss": float(eval_loss),
        "eval_acc": round(eval_acc, 4),
        "eval_precision": round(precision, 4),
        "eval_recall": round(recall, 4),
        "eval_f1": round(f1, 4),
        "tp": int(tp), "tn": int(tn), "fp": int(fp), "fn": int(fn)
    }
    
    return result

def test(args, model, tokenizer):
    """Enhanced testing function"""
    eval_dataset = TextDataset(tokenizer, args, args.test_data_file)

    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)
    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)
    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)

    if args.n_gpu > 1:
        model = torch.nn.DataParallel(model)

    logger.info("***** Running Test *****")
    logger.info("  Num examples = %d", len(eval_dataset))
    logger.info("  Batch size = %d", args.eval_batch_size)
    
    model.eval()
    probs = []
    labels = []
    
    test_bar = tqdm(eval_dataloader, desc="Testing", total=len(eval_dataloader))
    
    for batch_idx, batch in enumerate(test_bar):
        inputs = batch[0].to(args.device)
        label = batch[1].to(args.device)
        
        with torch.no_grad():
            prob = model(inputs)
            probs.append(prob.cpu().numpy())
            labels.append(label.cpu().numpy())
        
        test_bar.set_postfix({
            'batch': f'{batch_idx+1}/{len(eval_dataloader)}',
            'samples': f'{(batch_idx+1)*args.eval_batch_size}/{len(eval_dataset)}'
        })

    probs = np.concatenate(probs, 0)
    labels = np.concatenate(labels, 0)
    
    # Enhanced metrics calculation
    metrics = evaluate_predictions_enhanced(labels, probs)
    logger.info("Test Results:")
    for key, value in metrics.items():
        logger.info(f"  {key}: {value}")
    
    # Save results
    with open(os.path.join(args.output_dir, 'test_metrics.json'), 'w') as f:
        json.dump(metrics, f, indent=2)
    
    preds = probs[:, 0] > 0.5
    with open(os.path.join(args.output_dir, "predictions.txt"), 'w') as f:
        for example, pred in zip(eval_dataset.examples, preds):
            f.write(f"{example.idx}\t{int(pred)}\n")

def evaluate_predictions_enhanced(labels, probs):
    """Enhanced prediction evaluation with more metrics"""
    predicted_labels = (probs[:, 0] > 0.5).astype(int)
    
    # Basic metrics
    acc = float(accuracy_score(labels, predicted_labels))
    tn, fp, fn, tp = confusion_matrix(labels, predicted_labels, labels=[0, 1]).ravel()
    
    # Classification report
    reports = classification_report(labels, predicted_labels, target_names=['0', '1'], output_dict=True)
    
    # Advanced metrics
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    try:
        prc_auc = average_precision_score(labels, probs[:, 0])
    except:
        prc_auc = 0.0
    
    metrics = {
        'Accuracy': round(acc, 4),
        'Precision': round(precision, 4),
        'Recall': round(recall, 4),
        'F1_Score': round(f1, 4),
        'Specificity': round(specificity, 4),
        'PRC_AUC': round(prc_auc, 4),
        'TP': int(tp), 'TN': int(tn), 'FP': int(fp), 'FN': int(fn),
        'Pos_Precision': round(reports['1']['precision'], 4),
        'Pos_Recall': round(reports['1']['recall'], 4),
        'Pos_F1': round(reports['1']['f1-score'], 4),
        'Neg_Precision': round(reports['0']['precision'], 4),
        'Neg_Recall': round(reports['0']['recall'], 4),
        'Neg_F1': round(reports['0']['f1-score'], 4)
    }
    
    return metrics

def main():
    parser = argparse.ArgumentParser()

    # Required parameters
    parser.add_argument("--train_data_file", default=None, type=str, required=True,
                        help="The input training data file (a text file).")
    parser.add_argument("--output_dir", default=None, type=str, required=True,
                        help="The output directory where the model predictions and checkpoints will be written.")

    # Optional parameters
    parser.add_argument("--eval_data_file", default=None, type=str,
                        help="An optional input evaluation data file to evaluate the perplexity on (a text file).")
    parser.add_argument("--test_data_file", default=None, type=str,
                        help="An optional input evaluation data file to evaluate the perplexity on (a text file).")

    parser.add_argument("--model_type", default="roberta", type=str,
                        help="The model architecture to be fine-tuned.")
    parser.add_argument("--model_name_or_path", default=None, type=str,
                        help="The model checkpoint for weights initialization.")

    parser.add_argument("--config_name", default="", type=str,
                        help="Optional pretrained config name or path if not the same as model_name_or_path")
    parser.add_argument("--tokenizer_name", default="", type=str,
                        help="Optional pretrained tokenizer name or path if not the same as model_name_or_path")
    parser.add_argument("--cache_dir", default="", type=str,
                        help="Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)")
    parser.add_argument("--block_size", default=-1, type=int,
                        help="Optional input sequence length after tokenization.")
    parser.add_argument("--do_train", action='store_true',
                        help="Whether to run training.")
    parser.add_argument("--do_eval", action='store_true',
                        help="Whether to run eval on the dev set.")
    parser.add_argument("--do_test", action='store_true',
                        help="Whether to run eval on the dev set.")
    parser.add_argument("--evaluate_during_training", action='store_true',
                        help="Run evaluation during training at each logging step.")
    parser.add_argument("--do_lower_case", action='store_true',
                        help="Set this flag if you are using an uncased model.")

    parser.add_argument("--train_batch_size", default=4, type=int,
                        help="Batch size per GPU/CPU for training.")
    parser.add_argument("--eval_batch_size", default=4, type=int,
                        help="Batch size per GPU/CPU for evaluation.")
    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,
                        help="Number of updates steps to accumulate before performing a backward/update pass.")
    parser.add_argument("--learning_rate", default=5e-5, type=float,
                        help="The initial learning rate for Adam.")
    parser.add_argument("--weight_decay", default=0.0, type=float,
                        help="Weight deay if we apply some.")
    parser.add_argument("--adam_epsilon", default=1e-8, type=float,
                        help="Epsilon for Adam optimizer.")
    parser.add_argument("--max_grad_norm", default=1.0, type=float,
                        help="Max gradient norm.")
    parser.add_argument("--num_train_epochs", default=1.0, type=float,
                        help="Total number of training epochs to perform.")
    parser.add_argument("--max_steps", default=-1, type=int,
                        help="If > 0: set total number of training steps to perform. Override num_train_epochs.")
    parser.add_argument("--warmup_steps", default=0, type=int,
                        help="Linear warmup over warmup_steps.")

    parser.add_argument('--logging_steps', type=int, default=50,
                        help="Log every X updates steps.")
    parser.add_argument('--save_steps', type=int, default=50,
                        help="Save checkpoint every X updates steps.")
    parser.add_argument("--no_cuda", action='store_true',
                        help="Avoid using CUDA when available")
    parser.add_argument('--overwrite_output_dir', action='store_true',
                        help="Overwrite the content of the output directory")
    parser.add_argument('--seed', type=int, default=42,
                        help="random seed for initialization")
    parser.add_argument('--epoch', type=int, default=42,
                        help="random seed for initialization")
    parser.add_argument("--local_rank", type=int, default=-1,
                        help="For distributed training: local_rank")

    # Enhanced optimization parameters
    parser.add_argument('--use_weighted_sampling', action='store_true',
                        help="Use weighted sampling to handle class imbalance")
    parser.add_argument('--use_early_stopping', action='store_true',
                        help="Use early stopping")
    parser.add_argument('--patience', type=int, default=3,
                        help="Patience for early stopping")
    
    # Model enhancement parameters
    parser.add_argument('--pooling_strategy', type=str, default='attention',
                        choices=['cls', 'mean', 'max', 'attention', 'weighted'],
                        help="Pooling strategy for sequence representation")
    parser.add_argument('--dropout_rate', type=float, default=0.3,
                        help="Dropout rate for classifier")
    
    # Loss function parameters
    parser.add_argument('--use_focal_loss', action='store_true',
                        help="Use focal loss for class imbalance")
    parser.add_argument('--focal_alpha', type=float, default=2.0,
                        help="Alpha parameter for focal loss")
    parser.add_argument('--focal_gamma', type=float, default=2.0,
                        help="Gamma parameter for focal loss")
    parser.add_argument('--use_label_smoothing', action='store_true',
                        help="Use label smoothing")
    parser.add_argument('--label_smoothing', type=float, default=0.1,
                        help="Label smoothing factor")

    args = parser.parse_args()

    # Setup CUDA, GPU & distributed training
    if args.local_rank == -1 or args.no_cuda:
        device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
        args.n_gpu = torch.cuda.device_count()
    else:
        torch.cuda.set_device(args.local_rank)
        device = torch.device("cuda", args.local_rank)
        torch.distributed.init_process_group(backend='nccl')
        args.n_gpu = 1
        
    args.device = device
    args.per_gpu_train_batch_size = args.train_batch_size // max(1, args.n_gpu)
    args.per_gpu_eval_batch_size = args.eval_batch_size // max(1, args.n_gpu)

    # Setup logging
    logging.basicConfig(
        format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
        datefmt='%m/%d/%Y %H:%M:%S',
        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN
    )
    logger.warning(
        "Process rank: %s, device: %s, n_gpu: %s, distributed training: %s",
        args.local_rank, device, args.n_gpu, bool(args.local_rank != -1)
    )

    # Set seed
    set_seed(args.seed)

    # Load pretrained model and tokenizer
    if args.local_rank not in [-1, 0]:
        torch.distributed.barrier()

    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]
    config = config_class.from_pretrained(
        args.config_name if args.config_name else args.model_name_or_path,
        cache_dir=args.cache_dir if args.cache_dir else None
    )
    config.num_labels = 1
    
    tokenizer = tokenizer_class.from_pretrained(
        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,
        do_lower_case=args.do_lower_case,
        cache_dir=args.cache_dir if args.cache_dir else None
    )
    
    if args.block_size <= 0:
        args.block_size = tokenizer.max_len_single_sentence
    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)
    
    if args.model_name_or_path:
        encoder = model_class.from_pretrained(
            args.model_name_or_path,
            from_tf=bool('.ckpt' in args.model_name_or_path),
            config=config,
            cache_dir=args.cache_dir if args.cache_dir else None
        )
    else:
        encoder = model_class(config)

    # Use enhanced model
    model = EnhancedVulModel(encoder, config, tokenizer, args)
    
    if args.local_rank == 0:
        torch.distributed.barrier()

    logger.info("Training/evaluation parameters %s", args)

    # Training
    if args.do_train:
        if args.local_rank not in [-1, 0]:
            torch.distributed.barrier()

        train_dataset = TextDataset(tokenizer, args, args.train_data_file)

        if args.local_rank == 0:
            torch.distributed.barrier()

        train(args, train_dataset, model, tokenizer)

    # Evaluation
    if args.do_eval and args.local_rank in [-1, 0]:
        checkpoint_prefix = 'checkpoint-best-acc/model.bin'
        output_dir = os.path.join(args.output_dir, checkpoint_prefix)
        model.load_state_dict(torch.load(output_dir, map_location=args.device))
        model.to(args.device)
        result = evaluate(args, model, tokenizer)
        logger.info("***** Eval results *****")
        for key in sorted(result.keys()):
            logger.info("  %s = %s", key, str(result[key]))

    # Testing
    if args.do_test and args.local_rank in [-1, 0]:
        checkpoint_prefix = 'checkpoint-best-acc/model.bin'
        output_dir = os.path.join(args.output_dir, checkpoint_prefix)
        model.load_state_dict(torch.load(output_dir, map_location=args.device))
        model.to(args.device)
        test(args, model, tokenizer)

if __name__ == "__main__":
    main() 